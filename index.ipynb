{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (1.35.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.8.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\avola\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create an assistant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet creates a new \"assistant\" named \"Math Tutor\" \n",
    "# by calling the create method on the assistants object in the beta \n",
    "# section of the client, setting its instructions to \"You are a personal \n",
    "# math tutor. Write and run code to answer math questions,\" assigning it \n",
    "# the tool \"code_interpreter,\" and using the model \"gpt-3.5-turbo-16k.\"\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"Math Tutor\",\n",
    "    instructions = \"You are a personal math tutor. Write and run code to answer math questions\",\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    model = \"gpt-3.5-turbo-1106\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_AirZE4XHZvoXBYc3XP6YUIK0', created_at=1720059696, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "# The code snippet creates a new \"thread\" by calling the \n",
    "# create method on the threads object in the beta section \n",
    "# of the client, and then prints the details of the created thread.\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Add a message to a thread "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet creates a new \"message\" within a \n",
    "# specific \"thread\" by calling the create method, using \n",
    "# the thread ID, assigning the role as \"user,\" and setting \n",
    "# the content to \"Solve this problem 3x + 11 = 14.\"\n",
    "\n",
    "messages = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Solve this problem 3x + 11 = 14\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Run the assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet creates a new \"run\" within a \n",
    "# specific \"thread\" by calling the create method, \n",
    "# using the thread ID and assistant ID to set it up.\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_Ibcg0gwlhBE03vPLSThFOXPF', assistant_id='asst_6n6Kugctiu1gr7fQA4bxwotx', cancelled_at=None, completed_at=None, created_at=1720059697, expires_at=1720060297, failed_at=None, incomplete_details=None, instructions='You are a personal math tutor. Write and run code to answer math questions', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_AirZE4XHZvoXBYc3XP6YUIK0', tool_choice='auto', tools=[CodeInterpreterTool(type='code_interpreter')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "# The code prints the details of the newly created \"run.\"\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Display the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet gets details of an existing \n",
    "# \"run\" within a specific \"thread\" by calling the \n",
    "# retrieve method, using the thread ID and run ID to find it.\n",
    "\n",
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id= thread.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Solve this problem 3x + 11 = 14\n"
     ]
    }
   ],
   "source": [
    "# The code snippet prints each message's role and content in \n",
    "# reverse order from the messages.data list.\n",
    "\n",
    "for message in reversed(messages.data):\n",
    "    print(message.role + \": \" + message.content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload Files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"memgpt-paper.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    uploaded_file = client.files.create(\n",
    "        file=file,\n",
    "        purpose='assistants'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='memgpt-paper.pdf'>\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Upload files and add them to a Vector Store\n",
    "# https://platform.openai.com/docs/assistants/tools/file-search/step-2-upload-files-and-add-them-to-a-vector-store\n",
    "\n",
    "vector_store = client.beta.vector_stores.create(name=\"memgpt_research\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"memgpt-paper.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_iOl0FgCmwLmErLw8WZSOOA78', created_at=1720193793, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create a message within the created thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"How does memgpt allow LLMs to have unlimited context length?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Run the assistants response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id= thread.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: How does memgpt allow LLMs to have unlimited context length?\n",
      "assistant: The MemGPT (Memory-Augmented GPT) architecture allows Language Models (LMs) to have unlimited context length by leveraging a memory-augmented mechanism. In traditional transformer-based LMs like GPT, the context length is limited by the fixed window of attention, which means that each token can only attend to a fixed number of tokens in the input sequence. This limits the ability of the model to capture long-range dependencies and context.\n",
      "\n",
      "MemGPT overcomes this limitation by introducing a memory-augmented mechanism that enables the model to store and retrieve information from an external memory, effectively extending the context length beyond the fixed window of attention. This allows the model to access and utilize information from a much larger context, thereby enabling it to understand and generate longer and more coherent sequences of text.\n",
      "\n",
      "The memory-augmented mechanism in MemGPT typically involves integrating an external memory module, such as a key-value memory or a content-addressable memory, with the transformer architecture. This allows the model to dynamically store and access relevant information from the input sequence as needed, effectively providing the model with a form of \"memory\" that can store information beyond the immediate context.\n",
      "\n",
      "By leveraging this memory-augmented architecture, MemGPT enables LMs to effectively have unlimited context length, as it can access and incorporate information from the entire input sequence when making predictions or generating output. This leads to more coherent and context-aware generation, particularly for long and complex sequences of text.\n"
     ]
    }
   ],
   "source": [
    "for message in reversed(messages.data):\n",
    "    print(message.role + \": \" + message.content[0].text.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
